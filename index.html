<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="description" content="RSS 2016 Workshop - Representations">
    <meta name="author" content="Enric Galceran; Luca Carlone; Gian Diego Tipaldi; Liam Paull; Andrea Censi; Cesar Cadena">
    <link rel="icon" href="../../favicon.ico">

    <title>RSS 2016 Workshop: Geometry and Beyond: Representations, Physics, and Scene Understanding for Robotics</title>

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-theme.min.css" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <!-- Place this tag in your head or just before your close body tag. -->
    <script src="https://apis.google.com/js/platform.js" async defer></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Home</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#summary">Summary</a></li>
            <li><a href="#keynotes">Keynotes</a></li>
            <li><a href="#papers">Papers</a></li>
            <li><a href="#program">Program</a></li>
            <li><a href="#cfc">Call</a></li>
            <li><a href="#dates">Important Dates</a></li>
            <li><a href="#pc">Program Committee</a></li>
            <li><a href="#contact">Contact</a></li>
            <li><a href="https://plus.google.com/communities/102832228492942322585">G+ Community</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase">

      <div class="main-body">
        <h1>RSS 2016 Workshop:<br>Geometry and Beyond - Representations, Physics, and Scene Understanding for Robotics</h1>
<br>
        <figure>
            <img src="rss_logo.png" width="35%">
        </figure>

        <!-- style="color: white;" -->
        <h2 style="color: gray;">Sunday, June 19 2016</h2>
        <h3 style="color: gray;"><a href="https://www.google.com/maps/place/North+Quadrangle+Residential+and+Academic+Complex/@42.280738,-83.7401718,15z/data=!4m5!3m4!1s0x0:0xefd29040373a695a!8m2!3d42.280738!4d-83.7401718">2255 North Quad</a>, University of Michigan, Ann Arbor, MI</h3>
      </div>



      <!--summary-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="summary" style="padding-top: 70px; margin-top: -70px;">Summary</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
	  <p>This workshop brings together researchers from robotics, computer vision, computer graphics, machine learning, and AI, to discuss fundamental questions in robot perception: how should a robot represent the world? How can we design the best possible representation for a specific task? How can we unify geometric reconstruction and semantic mapping? Robotic perception has been so far dominated by mapping algorithms that build primitive geometric representations of the environment like occupancy grids, meshes, or point clouds. While the community has made great progress in making 3D reconstruction work in real world problems, current algorithms and representations are still unable to capture high-level structure in the environment. Existing mapping algorithms do not take into account the physics of the scene (e.g., pedestrians in a crowd typically walk on the ground, rather than fly), and decouple scene reconstruction from scene understanding, while a unified approach would be largely beneficial (e.g., identifying objects helps reasoning about their relations). The use of more expressive representations would impact many open problems including map compressions for long term navigation, robust data association, human robot interaction, and object manipulation. This workshop aims to bring forward the latest breakthroughs and cutting edge research on robot perception and map representations, and identify challenges and future research directions for the community.
	 </p>
          <p><b>Workshop structure:</b> We seek to answer these questions by identifying current key issues related to representation in robotics, and bringing researchers from other communities working on related topics such as scene understanding and representation itself that can provide tools to address those issues. Therefore, we plan to have invited speakers from two distinct groups: representation <b><i>users</i></b> and representation <b><i>providers</i></b> (see illustration below). Representation users are roboticists working on topics that could greatly benefit from more useful environment representations, like robotic navigation, manipulation, artificial intelligence, autonomous driving, or large-scale mapping. In turn, representation providers are researchers from fields such as computer vision, computational geometry and computer graphics, or machine learning.</p>

          <figure>
            <img src="ws_structure.png" width="600px">
            <!--<figcaption>The workshop program is organized around the idea of representation <i>users</i> and <i>providers</i>.</figcaption>-->
        </figure>

        <p>Join our discussions on our G+ community: <a href="https://plus.google.com/communities/102832228492942322585">The Problem of Mobile Sensors: Representations, Physics, and Scene Understanding for Robotics</a></p>

	<div class="g-community" data-width="340" data-href="https://plus.google.com/communities/102832228492942322585" data-layout="landscape"></div>
        </div>
      </div>



      <!--keynotes-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="keynotes" style="padding-top: 70px; margin-top: -70px;">Keynotes</h2>
        </div>
      </div>
	  <!--
      <div class="row">
        <div class="col-sm-12">
            <p><b>Dieter Fox</b>, University of Washington</p>
            <p><b>George Konidaris</b>, Duke University</p>
            <p><b>Oliver Brock</b>, Technical University of Berlin</p>
            <p><b>Pieter Abbeel</b>, UCBerkeley</p>
            <p><b>Ashutosh Saxena</b>, Cornell University and Stanford University</p>
            <p><b>John Laird</b>, University of Michigan</p>
            <p><b>Ingmar Posner</b>, University of Oxford</p>
        </div>
      </div>
	  -->
      <div class="row">
        <div class="col-sm-12">
		    <table>
			  <tr>
				<td><img src="./images/fox.jpg" width="150px"></td>
				<td><b>Dieter Fox</b>, University of Washington<br><i>Learning Intuitive Physics Models for Robots</i><br>The predominant approach to robot manipulation and related tasks is to use physics-based shape and interaction models of the robot, the environment, and the objects therein. While such models are well suited for trajectory planning and reasoning about the forces necessary to achieve certain goals, they require many quantities to be estimated from perceptual data, such as the location and mass of objects, surface frictions, and accurate 3D shape.  People, on the other hand, have an intuitive understanding of how things evolve over time and how to achieve certain changes. While this intuitive understanding might not be as accurate as computer-based physics models, it is fully grounded in a person's perceptual experience and therefore well suited for closed-loop control. In this talk I will discuss some thoughts and preliminary results on the challenges and possible benefits of learning and using intuitive physics models for robots.</td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/konidaris.jpg" width="150px"></td>
				<td><b>George Konidaris</b>, Duke University<br><i>What Are Representations For?</i><br>The question of the appropriate representations that will support intelligent behavior for robots (for manipulation, and in general) is a central question to the intelligent robotics enterprise. I will argue that the key question is "what are representations for?", and that by posing the question this way we can find well-supported answers.</td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/brock.jpg" width="150px"></td>
				<td><b>Oliver Brock</b>, TU Berlin<br><i>„Geometry is dead”</i><br>Hanif Kara said „Geometry is dead.” He refers to architecture and structural engineering.  In architecture (and computer graphics), researchers explore two alternatives to geometry: one with increased representational expressiveness (parametric surfaces) and one with minimalistic expressiveness (points). I think that in robotics we also started with geometry and recently turned to point clouds as an alternative. I will advocate that we should also explore the parametric route. By “parametric” I do not mean super-quadrics or things like that.  I mean parametric relationships in the combined space of action and sensor signals. I will argue that these relationships provide the most natural and adequate representations for manipulation actions and very naturally lead to competent robot manipulation and perception.</td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/abbeel.png" width="150px"></td>
				<td><b>Pieter Abbeel</b>, UCBerkeley<br><i>End-to-End Deep Learning of Representations for Robotics</i><br>Deep learning has enabled significant advances in supervised learning
problems such as speech recognition and visual recognition.  One of the
key characteristics of these advances is their end-to-end nature: a deep 
neural net is trained to map all the way from raw sensory inputs to classification.
In this talk I will highlight some recent results in end-to-end learning of
representations with direct applications in robotics: state estimation, 
visuomotor policies for manipulation and for flight, and inverse optimal control.</td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/saxena.jpg" width="150px"></td>
				<td><b>Ashutosh Saxena</b>, Cornell University and Stanford University<!--<br><i>TITLE</i><br>ABSTRACT.--></td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/laird.jpg" width="150px"></td>
				<td><b>John Laird</b> (presenting joint work with Aaron Mininger), University of Michigan<br><i>Integrating a Cognitive Architecture with Real-world Perception</i><br>A common pattern for perception in a robotic cognitive system is for a perceptual component to do its processing and produce a world state as input to a component responsible for the high-level reasoning, planning, and decision-making. This black-box approach to perception makes it difficult for the agent to handle errors or noise in its input, or to use its knowledge about the world to aid perception. In this work, we explore endowing a symbolic cognitive architecture (Soar) with a continuous spatial memory, so that an agent implemented in Soar can deliberately reason over perceptual updates and unifying them with an internal self-maintained world representation. During this unification, the agent brings to bear information from disparate sources including action knowledge, procedural knowledge, environmental regularities, and human interaction. Through experiments in a real-world robotic domain we show how the agent is robust to environmental noise and perceptual errors while tracking objects as it interacts with them, and how more knowledge available to the agent leads to better performance.</td>
			  </tr>
			  <tr height="20px"></tr>
			  <tr>
				<td><img src="./images/posner.jpg" width="150px"></td>
				<td><b>Ingmar Posner</b>, University of Oxford<br><i>Learning Representations for State Estimation and Perception</i><br>Situational awareness is a pivotal prerequisite for mobile autonomy. Traditionally, this is achieved by separately tackling the problem of object detection and tracking. This talk will motivate and describe Deep Tracking, an end-to-end approach to object tracking which directly maps from raw, partially occluded 2D laser sensor input to an un-occluded occupancy grid representation. I will describe the underpinnings of the original Deep Tracking approach as well as a more recent extension to apply it in complex, real-world environments. In addition I will also demonstrate how learning an underlying state representation allows for extension to another, related task by leveraging inductive transfer. Here this leads to a data efficient way of achieving effective semantic classification of objects in the scene via multi-task learning.</td>
			  </tr>
			</table>
        </div>
      </div>


      <!--papers/posters-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="papers" style="padding-top: 70px; margin-top: -70px;">Papers Presented at the Workshop</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
            <ul>
            <li>Helen Oleynikova, Alex Millane, Zachary Taylor, Enric Galceran, Juan Nieto, and Roland Siegwart:<br />
            <a href="papers/BeyondGeometryRSSW16_2_CameraReadySubmission_Oleynikova.pdf">Signed Distance Fields: A Natural Representation for Both Mapping and Planning</a></li>
            <li>Lina María Paz, Tarlan Suleymanov, Pedro Piniés, Geoff Hester, and Paul Newman:<br />
            <a href="papers/BeyondGeometryRSSW16_3_CameraReadySubmission_Paz.pdf">On-line Scene Understanding for Closed Loop Control</a></li>
            <li>Michael Tanner, Pedro Piniés, Lina María Paz, and Paul Newman:<br />
            <a href="papers/BeyondGeometryRSSW16_4_CameraReadySubmission_Tanner.pdf">Keep Geometry in Context: Using Contextual Priors for Very-Large-Scale 3D Dense Reconstructions</a></li>
            <li>Tomasz Piotr Kucner, Martin Magnusson, Erik Schaffernicht, Victor Hernandez Bennetts, and Achim J. Lilienthal:<br />
            <a href="papers/BeyondGeometryRSSW16_5_CameraReadySubmission_Kucner.pdf">Tell me about dynamics! Mapping velocity fields from sparse samples with Semi-Wrapped Gaussian Mixture Models</a></li>
            <li>Marcin Dymczyk, Igor Gilitschenski, Roland Siegwart, and Elena Stumm:<br />
            <a href="papers/BeyondGeometryRSSW16_6_CameraReadySubmission_Dymczyk.pdf">Map Summarization for Tractable Lifelong Mapping</a></li>
            <li>Kevin Eckenhoff, Liam Paull, and Guoquan Huang:<br />
            <a href="papers/BeyondGeometryRSSW16_7_CameraReadySubmission_Eckenhoff.pdf">Decoupled, Consistent Node Removal and Edge Sparsification for Graph-based SLAM</a></li>
            <li>Benjamin Burchfiel and George Konidaris:<br />
            <a href="papers/BeyondGeometryRSSW16_8_CameraReadySubmission_Burchfiel.pdf">Generalized 3D Object Representation using Bayesian Eigenobjects</a></li>
            <li>Md. Alimoor Reza and Jana Košecká:<br />
            <a href="papers/BeyondGeometryRSSW16_9_CameraReadySubmission_Reza.pdf">Reinforcement Learning for Semantic Segmentation in Indoor Scenes</a></li>
            <li>Lawson L.S. Wong:<br />
            <a href="papers/BeyondGeometryRSSW16_10_CameraReadySubmission_Wong.pdf">Object-based World Modeling for Mobile-Manipulation Robots</a></li>
            <li>Karthik Desingh, Mehran Maghoumi, Odest Chadwicke Jenkins, Joseph J. LaViola, and Lionel Reveret:<br />
            <a href="papers/BeyondGeometryRSSW16_13_CameraReadySubmission_Desingh.pdf">Object Manipulation in Cluttered Scenes Informed by Physics and Sketching</a></li>
            <li>John Oberlin and Stefanie Tellex:<br />
            <a href="papers/BeyondGeometryRSSW16_14_CameraReadySubmission_Oberlin.pdf">Time-Lapse Light Field Photography With a 7 DoF Arm</a></li>
            <li style="color: white;" />
            </ul>

        </div>
      </div>



      <!--program-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="program" style="padding-top: 70px; margin-top: -70px;">Program</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
          <p>The morning session focuses on representation users, while the afternoon session focuses on representation providers (as defined above).
	  </p>

          <table>
              <tr bgcolor="#DDDDDD">
                  <td>Time</td>
                  <td>Activity</td>
              </tr>
              <tr>
                  <td>9:00 - 9:05</td>
                  <td>Welcome and introduction</td>
              </tr>
              <tr>
                  <td>9:05 - 9:35</td>
                  <td> Keynote #1 - <b>Dieter Fox</b>
                  </td>
              </tr>
              <tr>
                  <td>9:35 - 10:05</td>
                  <td> Keynote #2 - <b>Ingmar Posner</b>
                  </td>
              </tr>
              <tr>
                  <td>10:05 - 10:30</td>
                  <td> COFFEE BREAK
                  </td>
              </tr>
              <tr>
                  <td>10:30 - 11:00</td>
                  <td> Poster Spotlights
                  </td>
              </tr>
              <tr>
                  <td>11:00 - 11:30</td>
                  <td> Keynote #3 - <b>George Konidaris</b>
                  </td>
              </tr>
              <tr>
                  <td>11:30 - 12:00</td>
                  <td> Keynote #4 - <b>Ashutosh Saxena</b>
                  </td>
              </tr>
              <tr>
                  <td>12:00 - 12:15</td>
                  <td> Morning Wrap-up
                  </td>
              </tr>
              <tr>
                  <td>12:15 - 2:00</td>
                  <td> LUNCH
                  </td>
              </tr>
              <tr>
                  <td>2:00 - 2:30</td>
                  <td> Keynote #5 - <b>John Laird</b>
                  </td>
              </tr>
              <tr>
                  <td>2:30 - 3:00</td>
                  <td> Keynote #6 - <b>Oliver Brock</b>
                  </td>
              </tr>
              <tr>
                  <td>3:00 - 3:30</td>
                  <td> Poster Session
                  </td>
              </tr>
              <tr>
                  <td>3:30 - 4:00</td>
                  <td> COFFEE BREAK
                  </td>
              </tr>
              <tr>
                  <td>4:00 - 4:30</td>
                  <td> Keynote #7 - <b>Pieter Abbeel</b>
                  </td>
              </tr>
              <tr>
                  <td>4:30 - 5:30</td>
                  <td> Panel Discussion. Guest moderator: <b>Edwin Olson</b>
                  </td>
              </tr>
          </table>
        </div>
      </div>



      <!--call for contributions-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="cfc" style="padding-top: 70px; margin-top: -70px;">Call for Contributions</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
            <p>Please make your submission, before <b>April 29 2016</b>, at: <b><a href="https://cmt3.research.microsoft.com/RSSWGB2016">https://cmt3.research.microsoft.com/RSSWGB2016</a></b></p>
            <p>We are soliciting extended abstracts/full papers describing novel
            work in robot representations. Submitted papers will be reviewed by
            the <a href="#pc">program committee</a>. Accepted submissions will give
            a short spotlight talk and present their work in poster format.</p>

            <p>Submissions should conform to standard RSS formatting, up to 6 pages
            excluding references. Accepted papers will be published on the workshop
            website.</p>

            <p>Suitable topics include, but are not limited to:</p>

            <ul>
            <li>New environment and map representations</li>
            <li>Map summarization / map compression for long-term navigation</li>
            <li>Scene understanding</li>
            <li>Task-oriented environment modeling</li>
            <li>Object recognition for manipulation and interaction</li>
            <li>Semantic localization and mapping</li>
            <li>Continuous or alternative map and trajectory representations</li>
            </ul>

        </div>
      </div>


      <!--Important dates-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="dates" style="padding-top: 70px; margin-top: -70px;">Important Dates</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
            <p><b>Submission Due:</b> <strike>April 15, 2016</strike> <b>April 29, 2016</b></p>
            <p><b>Acceptance Notification:</b> <strike>May 20, 2016</strike> <b>May 23, 2016</b></p>
            <p><b>Camera-ready Papers Due:</b> June 3, 2016</p>
            <p><b>Workshop:</b> June 19, 2016</p>
        </div>
      </div>


      <!--program committee-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="pc" style="padding-top: 70px; margin-top: -70px;">Program Committee</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
            <p><b>Jason Stack</b>, ONR Program Manager </p>
            <p><b>John McDonald</b>, Maynooth University </p>
            <p><b>Yasir Latif</b>, The University of Adelaide </p>
            <p><b>Jeff Walls</b>, University of Michigan </p>
            <p><b>Alejandro Rituerto</b>, The Smith-Kettlewell Eye Research Institute </p>
            <p><b>Andreas Geiger</b>, Max Planck Institute </p>
            <p><b>Sunando Sengupta</b>, Vicon Motion Capture Systems </p>
            <p><b>Pedro Piniés</b>, University of Oxford </p>
            <p><b>Lina M. Paz</b>, University of Oxford </p>
            <p><b>Elena Stumm</b>, ETH Zurich </p>
        </div>
      </div>

      <!--Contact-->
      <div class="row page-header">
        <div class="col-sm-12">
          <h2 id="contact" style="padding-top: 70px; margin-top: -70px;">Contact</h2>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
            <p>Enric Galceran, ETH Zurich</br>
            Email: enricg@ethz.ch</br>
            <a href="http://robots.engin.umich.edu/~egalcera/">Web</a></p>
            <p>Luca Carlone, MIT</br>
            Email: lcarlone@mit.edu</br>
            <a href="http://www.lucacarlone.com">Web</a></p>
            <p>Gian Diego Tipaldi, University of Freiburg</br>
            Email: tipaldi@cs.uni-freiburg.de</br>
            <a href="http://www2.informatik.uni-freiburg.de/~tipaldi/home.html">Web</a></p>
            <p>Liam Paull, MIT</br>
            Email: lpaull@csail.mit.edu</br>
            <a href="http://people.csail.mit.edu/lpaull">Web</a></p>
            <p>Andrea Censi, MIT</br>
            Email: censi@mit.edu</br>
            <a href="http://censi.mit.edu">Web</a></p>
            <p>Cesar Cadena, ETH Zurich</br>
            Email: cesarc@ethz.ch</br>
            <a href="http://cs.adelaide.edu.au/~cesar/">Web</a></p>
        </div>
      </div>


    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
